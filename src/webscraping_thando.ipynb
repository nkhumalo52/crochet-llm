{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /Users/nkhumalo/VSCode/webscrape/lib/python3.11/site-packages (4.18.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /Users/nkhumalo/VSCode/webscrape/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.1)\n",
      "Requirement already satisfied: trio~=0.17 in /Users/nkhumalo/VSCode/webscrape/lib/python3.11/site-packages (from selenium) (0.25.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /Users/nkhumalo/VSCode/webscrape/lib/python3.11/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /Users/nkhumalo/VSCode/webscrape/lib/python3.11/site-packages (from selenium) (2024.2.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /Users/nkhumalo/VSCode/webscrape/lib/python3.11/site-packages (from selenium) (4.10.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /Users/nkhumalo/VSCode/webscrape/lib/python3.11/site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in /Users/nkhumalo/VSCode/webscrape/lib/python3.11/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /Users/nkhumalo/VSCode/webscrape/lib/python3.11/site-packages (from trio~=0.17->selenium) (3.6)\n",
      "Requirement already satisfied: outcome in /Users/nkhumalo/VSCode/webscrape/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /Users/nkhumalo/VSCode/webscrape/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in /Users/nkhumalo/VSCode/webscrape/lib/python3.11/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /Users/nkhumalo/VSCode/webscrape/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Users/nkhumalo/VSCode/webscrape/lib/python3.11/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import queue\n",
    "import threading\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProxyManager:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            \"Accept\": \"*/*\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "\n",
    "    def get_proxies(self):\n",
    "        self.proxies = queue.Queue()\n",
    "        \n",
    "        url = \"https://free-proxy-list.net\"\n",
    "        r = requests.get(url, headers=self.headers)\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        table_rows = soup.find(\"section\", attrs={\"id\": \"list\"}).find(\"tbody\").find_all(\"tr\")\n",
    "\n",
    "        for row in table_rows:\n",
    "            ip_address = row.find_all(\"td\")[0].text.strip()\n",
    "            port = row.find_all(\"td\")[1].text.strip()\n",
    "\n",
    "            if row.find_all(\"td\")[6].text.strip() == \"yes\":\n",
    "                self.proxies.put(f\"{ip_address}:{port}\")\n",
    "\n",
    "    def check_proxies(self):\n",
    "        self.valid_proxies = []\n",
    "\n",
    "        while not self.proxies.empty():\n",
    "            proxy = self.proxies.get()\n",
    "\n",
    "            try:\n",
    "                r = requests.get(\"https://www.google.com\", headers=self.headers, proxies={\"http\": proxy, \"https\": proxy}, timeout=5)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            if r.status_code == 200:\n",
    "                self.valid_proxies.append(proxy)\n",
    "\n",
    "    def get_proxy_list(self):\n",
    "        while True:\n",
    "            try:\n",
    "                self.get_proxies()\n",
    "\n",
    "                threads = [threading.Thread(target=self.check_proxies) for _ in range(32)]\n",
    "\n",
    "                [t.start() for t in threads]\n",
    "                [t.join() for t in threads]\n",
    "\n",
    "                assert len(self.valid_proxies) > 0\n",
    "            \n",
    "                return self.valid_proxies\n",
    "            except:\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_driver(proxy=None):\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    if proxy:\n",
    "        chrome_options.add_argument(f'--proxy-server={proxy}')\n",
    "    chrome_options.add_argument(\"--lang=en-US\")\n",
    "    chrome_options.add_argument(\"--start-maximized\")\n",
    "    chrome_service = webdriver.ChromeService(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=chrome_service, options=chrome_options)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_table_data(driver, url, proxy_manager):\n",
    "    proxy = random.choice(proxy_manager.get_proxy_list())\n",
    "    driver = setup_driver(proxy)\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"tabProcedura\")))\n",
    "    links = driver.find_elements(By.XPATH, \"//table[@id='tabProcedura']/tbody/tr/td[2]/a\")\n",
    "    data = []\n",
    "    for link in links:\n",
    "        link_url = link.get_attribute(\"href\")\n",
    "        link_text = link.text\n",
    "        link.click()\n",
    "        time.sleep(3)  # Adjust the delay as needed for the table to load\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        table = soup.find(\"table\", id=\"elenco\")\n",
    "        headers = [th.text for th in table.find_all(\"th\")]\n",
    "        rows = []\n",
    "        for row in table.find_all(\"tbody/tr\"):\n",
    "            row_data = [td.text for td in row.find_all(\"td\")]\n",
    "            rows.append(row_data)\n",
    "        data.append({\n",
    "            \"Link\": link_url,\n",
    "            \"Text\": link_text,\n",
    "            \"Table Headers\": headers,\n",
    "            \"Table Rows\": rows\n",
    "        })\n",
    "        driver.back()\n",
    "        time.sleep(3)  # Adjust the delay as needed\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscraped_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 34\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     url \u001b[38;5;241m=\u001b[39m url_base \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(page_number)\n\u001b[0;32m----> 9\u001b[0m     proxy \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproxy_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_proxies\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Get a random proxy for each page\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     driver \u001b[38;5;241m=\u001b[39m setup_driver(proxy)\n\u001b[1;32m     11\u001b[0m     driver\u001b[38;5;241m.\u001b[39mget(url)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/random.py:372\u001b[0m, in \u001b[0;36mRandom.choice\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Choose a random element from a non-empty sequence.\"\"\"\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# As an accommodation for NumPy, we don't use \"if not seq\"\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# because bool(numpy.array()) raises a ValueError.\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot choose from an empty sequence\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m seq[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_randbelow(\u001b[38;5;28mlen\u001b[39m(seq))]\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    proxy_manager = ProxyManager()\n",
    "    url_base = \"https://www.portalecreditori.it/procedure.php?filter=txtFAnno%7C2021&order=procedura&verso=asc&page=\"\n",
    "    page_number = 1\n",
    "    scraped_data = []\n",
    "\n",
    "    while True:\n",
    "        url = url_base + str(page_number)\n",
    "        proxy = random.choice(proxy_manager.get_proxies())  # Get a random proxy for each page\n",
    "        driver = setup_driver(proxy)\n",
    "        driver.get(url)\n",
    "        \n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"tabProcedura\")))\n",
    "            data = scrape_table_data(driver, url)\n",
    "            scraped_data.extend(data)\n",
    "            \n",
    "            # Increment page number for the next iteration\n",
    "            page_number += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred on page {page_number}: {str(e)}\")\n",
    "            break\n",
    "        \n",
    "        driver.quit()  # Close the driver after scraping each page\n",
    "        time.sleep(random.uniform(1, 3))  # Add a random delay between requests to avoid being blocked\n",
    "\n",
    "    # Convert scraped data to DataFrame\n",
    "    df = pd.DataFrame(scraped_data)\n",
    "    \n",
    "    # Save DataFrame to CSV\n",
    "    df.to_csv(\"scraped_data.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
